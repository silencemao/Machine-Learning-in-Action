{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "加载训练数据, postingList是所有的训练集, 每一个列表代表一条言论, 一共有8条言论\n",
    "            classVec代表每一条言论的类别, 0是正常, 1是有侮辱性\n",
    "            返回 言论和类别\n",
    "'''\n",
    "def loadDataSet():\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], \n",
    "                  ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], \n",
    "                  ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'hime'], \n",
    "                  ['stop', 'posting', 'stupid', 'worthless', 'garbage'], \n",
    "                  ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], \n",
    "                  ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]\n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "创建词汇表, 就是把这个文档中所有的单词不重复的放在一个列表里面\n",
    "'''\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])           # 新建一个set集合, 保证里面的数据不重复\n",
    "    for document in dataSet:     # 获得每一个文档\n",
    "        vocabSet = vocabSet | set(document)   # 将这个文档去重之后和词汇表求并集\n",
    "    return list(vocabSet)                     # 将词汇表转换为列表返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "vocabList是由createVocabList产生的词汇表\n",
    "inputSet是输入新的文档\n",
    "'''\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)  # 生成一个全0列表, 个数为输入文档的长度\n",
    "    for word in inputSet:             # 遍历输入文档中的每一个单词\n",
    "        if word in vocabList:         # 如果这个单词在词汇表中\n",
    "            returnVec[vocabList.index(word)] = 1  # 列表中该位置置1\n",
    "        else:                                     # 否则依然为0\n",
    "            print(\"the word %s is not in my Vocabulary\" % word) \n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listOPosts, listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       "  ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       "  ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'hime'],\n",
       "  ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       "  ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       "  ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']],\n",
       " [0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOPosts, listClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myVocabList = createVocabList(listOPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['problems', 'dog', 'please', 'is', 'him', 'stop', 'licks', 'not', 'food', 'maybe', 'my', 'steak', 'stupid', 'love', 'posting', 'to', 'hime', 'park', 'garbage', 'ate', 'take', 'buying', 'I', 'cute', 'mr', 'quit', 'help', 'how', 'dalmation', 'has', 'worthless', 'flea', 'so']\n"
     ]
    }
   ],
   "source": [
    " print(myVocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']\n",
      "[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(listOPosts[0])\n",
    "print(setOfWords2Vec(myVocabList, listOPosts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "计算先验概率\n",
    "trainMatrix: 词向量矩阵\n",
    "trainCategory: 每一个词向量的类别\n",
    "返回每一个单词属于侮辱性和非侮辱性词汇的先验概率, 以及训练集包含侮辱性文档的概率\n",
    "'''\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)    # 由训练集生成的词向量矩阵\n",
    "    numWords = len(trainMatrix[0])     # 每一个词向量的长度\n",
    "    pAbusive  = sum(trainCategory) / float(numTrainDocs)    # 计算侮辱性文档的先验概率\n",
    "    p0Num = ones(numWords)             # 生成全1 array, 长度为词向量的长度, 用于统计每个单词在整个矩阵中出现的次数(分子)\n",
    "    p1Num = ones(numWords)\n",
    "    p0Denom = 2.0                      # 初始化为2(分母), 拉普拉斯平滑\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):      # 遍历每一个词向量\n",
    "        if trainCategory[i] == 1:      # 如果该词向量的类别为1\n",
    "            p1Num += trainMatrix[i]    # 计算P(x0) P(x1) P(xn)\n",
    "            p1Denom += 1               # 统计侮辱性文档的个数\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]    # 计算P(x0) P(x1) P(xn)\n",
    "            p0Denom += 1               # 统计非侮辱性的文档个数\n",
    "    p0Vect = log(p0Num / p0Denom)      # 计算P(x0|0) P(x1|0) P(xn|0)\n",
    "    p1Vect = log(p1Num / p1Denom)      # 计算P(x0|1) P(x1|1) P(xn|1)   取对数是防止多个小数相乘出现下溢\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "制作词向量矩阵\n",
    "将每一个文档转换为词向量, 然后放入矩阵中\n",
    "'''\n",
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.91629073, -0.91629073, -0.91629073, -0.91629073, -0.91629073,\n",
       "        -0.91629073, -0.91629073, -1.60943791, -1.60943791, -1.60943791,\n",
       "        -0.22314355, -0.91629073, -1.60943791, -0.91629073, -1.60943791,\n",
       "        -0.91629073, -0.91629073, -1.60943791, -1.60943791, -0.91629073,\n",
       "        -1.60943791, -1.60943791, -0.91629073, -0.91629073, -0.91629073,\n",
       "        -1.60943791, -0.91629073, -0.91629073, -0.91629073, -0.91629073,\n",
       "        -1.60943791, -0.91629073, -0.91629073]),\n",
       " array([-1.60943791, -0.51082562, -1.60943791, -1.60943791, -0.91629073,\n",
       "        -0.91629073, -1.60943791, -0.91629073, -0.91629073, -0.91629073,\n",
       "        -1.60943791, -1.60943791, -0.22314355, -1.60943791, -0.91629073,\n",
       "        -0.91629073, -1.60943791, -0.91629073, -0.91629073, -1.60943791,\n",
       "        -0.91629073, -0.91629073, -1.60943791, -1.60943791, -1.60943791,\n",
       "        -0.91629073, -1.60943791, -1.60943791, -1.60943791, -1.60943791,\n",
       "        -0.51082562, -1.60943791, -1.60943791]),\n",
       " 0.5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)\n",
    "p0V, p1V, pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "制作贝叶斯分类器\n",
    "vec2Classify: 测试样本的词向量\n",
    "p0Vec: P(x0|Y=0) P(x1|Y=0) P(xn|Y=0)\n",
    "p1Vec: P(x0|Y=1) P(x1|Y=1) P(xn|Y=1)\n",
    "pClass1: P(y)\n",
    "# log(P(x1|1)*P(x2|1)*P(x3|1)P(1))=log(P(x1|1))+log(P(x2|1))+log(P(1))\n",
    "'''\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)       \n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "测试贝叶斯分类器\n",
    "'''\n",
    "def testingNB():\n",
    "    listOPosts, listClasses =  loadDataSet()       # 加载数据\n",
    "    myVocabList = createVocabList(listOPosts)      # 制作词汇表\n",
    "    trainMat = []                                  # 制作训练集词向量\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V, p1V, pAb = trainNB0(trainMat, listClasses) # 计算先验概率\n",
    "    testEntry = ['love', 'my', 'dalmation']         # 测试文档1\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as :', classifyNB(thisDoc, p0V, p1V, pAb))\n",
    "    testEntry = ['stupid', 'garbage', 'stupid']     # 测试文档2\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as : ', classifyNB(thisDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
